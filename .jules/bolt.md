## 2024-10-18 - Hoisting Positional Embeddings in DPTHead **Learning:** Recomputing constant tensors (like positional embeddings) inside nested loops (batch chunks + pyramid levels) can add significant overhead, even if the individual computation seems light (sin/cos/einsum). **Action:** Always look for invariant computations inside loops, especially those involving tensor creation or math, and hoist them out. Check if `batch_size` is split into chunks, as this multiplies the overhead.
